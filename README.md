# AISI

Concepts:
- [x] Transformers
- [x] Shallow heuristics
- [x] Narrow training distribution

Points of interest:
- [x] Ability to 'reason'
- [x] Different steps of training
- [x] Transfer learning

# TinyStories

LMs: factual knowledge + reasoning + contextual tracking.

## Synthetic dataset generated by GPT-3.5 and GPT-4

- [x] useful to train and evaluate __Language Models (LMs) smaller__ than the state-of-the-art models (below 10 million total parameters (SLMs)) or have much simpler architectures (with only one transformer block).
- [x] demonstrates __reasoning capabilities__.

## Suggestion of a new framework using GPT-4

- [x] __multidimensional score__ for the model (grammar, creativity, instruction-following) =/= from very structured standard benchmarks.
- [x] useful for __low-resource / specialized domains teams__ + provides a __new perspective on the capabilities__ of LMs

## Computational efficiency and behavior of SLMs

- [x] generative models trained on TinyStories show __similar behaviors__ to Larger Language Models (LLMs).
- [x] conducting extensive experiments on different hyperparameters, architectures, and training methods reveals __insights into the performance and quality__ of these models even with limited computational resources.

## Improved interpretabilty of SLMs

- [x] models trained on TinyStories appear to be substantially __more interpretable__ than larger ones, with clear attention patterns and meaningful neuron activations.
- [x] visualization and analysis of attention and activation maps provide __insights into the generation process__ and story content, enhancing our understanding of how these models operate.

## Comparison with Larger Models

- [x] models trained on TinyStories can produce __results comparable to much larger models__ like GPT2-XL, demonstrating the effectiveness of this approach in __generating high-quality text__.

# Concepts 

## Transformer

- [x] introduced in [Attention is all you need](https://arxiv.org/abs/1706.03762)
- [x] __neural network architecture__ primarily used for __natural language processing__.
- [x] key feature: __attention mechanism__, allowing it to capture complex relationships in sequential data.
- [x] excel in tasks like __machine translation and text generation__. 
- [x] famous models such as GPT and BERT employ transformer architectures.

## Shallow neuristics

- [x] __simple rules or low-complexity methods__ used to solve problems or make decisions. 
- [x] can be quick to apply but may also yield __approximate results__.

# Points of interest

## Steps of LMs' training

1. __Data collection__.
2. __Preprocessing:__ cleaning and formatting the text data, including __tokenization__ (breaking text into individual words or subwords), handling special characters, and potentially applying techniques like stemming or lemmatization.
    a. __stemming:__ reducing words to their root by removing suffixes. e.g. "eating", "eats", "eaten" become "eat". Fast but may produce imperfect results as it does not consider context.
    b. __lemmatization:__ reducing words to their canonical form. e.g. "better" become "good", "running" become "run".
3. __Token embedding:__ converting tokens into __vectors__ that can be understood by the model. Involves techniques like __word embeddings__ (e.g., Word2Vec, GloVe) or __subword embeddings__ (e.g., Byte Pair Encoding, SentencePiece).
4. __Model training:__ involves feeding the tokenized and embedded text into the model and adjusting its parameters (e.g., weights in neural networks) iteratively to minimize prediction errors.
    a. __optimization algorithms__: improve the model's __ability to generate coherent and contextually relevant text__, while minimizing errors and maximizing language understanding. 
5. __Evaluation:__ assessing the performance of the trained model using various metrics such as __perplexity, accuracy, or BLEU score__ (Bilingual Evaluation Understudy).
6. __Fine-tuning__ (optional): fine-tuning the pre-trained model on a __specific task or domain__ to improve its performance for a particular application.
7. __Deployment__.

## Transfer learning

- [x] transferring a pre-trained model's learned knowledge to a new related task or domain.
- [x] pre-trained model is used as a starting point.
- [x] allows saving time + resources.

# Tasks

## Task 1: Create a synthetic dataset

Generate a train (size: 70) & a test dataset (size: 30):
```
    Dtrain = { (xi, yi): 1 <= i <= 100 ^ (i % 10) \notin { 1, 3, 7 }}
    Dtest = { (xi, yi): 1 <= i <= 100 ^ (i % 10) \in { 1, 3, 7 }}
```

<!-- ##### Train set
- [ ] various instruction / sentence
- [ ] various instruction's length
- [ ] add noise + mistakes (e.g. spelling mistakes)
- [ ] reverse order

##### Test set -->

__Difficulty: ‚≠ê__
__Duration: 30 minutes__

! Decode the tokenizer.pad_token to add it to our synthetic completions.

üìö Doc:
- [Hugging Face ü§ó Create dataset](https://huggingface.co/docs/datasets/create_dataset)
- stackoverflow

## Task 2: Evaluate models

Reasons for avoiding generate() function:
- [x] Customization of the generation process
- [x] Performance
- [x] Flexibility: ability to add additional features of custom preprocessing steps to generation process
- [x] Control over model + behavior
- [x] [If there are needs during inference and training stages (generate() can only be used at inference time](https://discuss.huggingface.co/t/what-is-the-difference-between-forward-and-generate/10235)

#### How does it work?
- Create a dataset_batches where the step=batch_size. Advantages = optimization+parallelism, PyTorch/TensorFlow are optimized for processing batches of data in parallel.
- Iterate on batches and for each get prompts/completions.

#### Gradients activation/desactivation
- Activation:
    - [x] Training stage -> learn model params
    - [x] Calculated + used to adjust model params to minimize the loss on the training data
- Desactivation:
    - [x] Inference / evaluation stages
    - [x] No parameter adjustments are made because the parameters have already been learned during training.

__Difficulty: ‚≠ê‚≠ê__
__Duration: 1h__

üìö Doc:
- [Hugging face ü§ó Evaluate predictions](https://huggingface.co/docs/datasets/metrics)
- [Hugging face ü§ó Utilities for Tokenizers: understand PreTrainedTokenizerBase, params + returns](https://huggingface.co/transformers/v4.0.1/internal/tokenization_utils.html)
- [Hugging Face ü§ó Inference](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client)
- [Inference PyTorch Models](https://onnxruntime.ai/docs/tutorials/accelerate-pytorch/pytorch.html)

## Task 3: Test your evaluator

1. Init DummyModel class constructor
2. Implement a customized forward method

__Difficulty: ‚≠ê‚≠ê‚≠ê‚≠ê__
__Duration: 2h__

üìö Doc:
- [x] [Hugging face ü§ó GPT Neo](https://huggingface.co/docs/transformers/model_doc/gpt_neo)

## Task 4: Transfer Learning

__Difficulty: ‚≠ê‚≠ê‚≠ê__
__Duration: 1h__

Hyperparameters: set to control the training process + can influence performance of the model. It can be:
- learning rate
- batch size
- number of epochs
- optimizer
- regularization parameters
- dropout rate
- model architecture choices: number of layers in a neural network, number of neurons/layer, ...

What I tried:
- change learning rate, epochs, batch_size

üìö Doc:
- [x] [Hugging face ü§ó Causal language modeling | Train](https://huggingface.co/docs/transformers/tasks/language_modeling)
- [x] [Hugging face ü§ó Evaluate - A library for easily evaluating machine learning models and datasets](https://huggingface.co/docs/evaluate/index)
- [x] [Hugging face ü§ó Evaluate - transformers](https://huggingface.co/docs/evaluate/transformers_integrations)
- [x] [Stackoverflow - Using huggingface transformers trainer method for hugging face datasets](https://stackoverflow.com/questions/74223324/using-huggingface-transformers-trainer-method-for-hugging-face-datasets)
- [x] [Error using transformers Trainer - remove_unused_columns=False](https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/24)
- [x] [PyTorch üî• torch.optim](https://pytorch.org/docs/stable/optim.html)

## Task 5: Have we tested our hypothesis?

#### Other experiment
- Exploring a task that requires a balance between the nature of the words: nouns, verbs, adjectives, numbers.

#### Code's improvements
- Task 1: OK
- Task 2: truncate completions to improve DummyModel's accuracy
- Task 3: 
    - constructor
    - search more if forward params are useful
    - logits instantiation / initialization / storage
- Task 4: develop training stage

#### Execution of this experiment
- See the impact of a different dataset (size & quality)
- Test hypothesis with differents SLMs

Doc:
- [x] [Tiny but mighty: The Phi-3 small language models with big potential](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/)

__Difficulty: ‚≠ê__
__Duration: 30 minutes__

## Task 6: [Optional] Explore

<!-- # Improvements -->

