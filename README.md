# AISI

Concepts:
- [x] Transformers
- [x] Shallow heuristics
- [x] Narrow training distribution

Points of interest:
- [x] Ability to 'reason'
- [x] Different steps of training
- [x] Transfer learning

# TinyStories

## Synthetic dataset generated by GPT-3.5 and GPT-4

- [x] useful to train and evaluate __Language Models (LMs) smaller__ than the state-of-the-art models (below 10 million total parameters (SLMs)) or have much simpler architectures (with only one transformer block).
- [x] demonstrates __reasoning capabilities__.

## Suggestion of a new framework using GPT-4

- [x] __multidimensional score__ for the model (grammar, creativity, instruction-following) =/= from very structured standard benchmarks.
- [x] useful for __low-resource / specialized domains teams__ + provides a __new perspective on the capabilities__ of LMs

## Computational efficiency and behavior of SLMs

- [x] generative models trained on TinyStories show __similar behaviors__ to Larger Language Models (LLMs).
- [x] conducting extensive experiments on different hyperparameters, architectures, and training methods reveals __insights into the performance and quality__ of these models even with limited computational resources.

## Improved interpretabilty of SLMs

- [x] models trained on TinyStories appear to be substantially __more interpretable__ than larger ones, with clear attention patterns and meaningful neuron activations.
- [x] visualization and analysis of attention and activation maps provide __insights into the generation process__ and story content, enhancing our understanding of how these models operate.

## Comparison with Larger Models

- [x] models trained on TinyStories can produce __results comparable to much larger models__ like GPT2-XL, demonstrating the effectiveness of this approach in __generating high-quality text__.

# Concepts 

## Transformer

- [x] introduced in [Attention is all you need](https://arxiv.org/abs/1706.03762)
- [x] __neural network architecture__ primarily used for __natural language processing__.
- [x] key feature: __attention mechanism__, allowing it to capture complex relationships in sequential data.
- [x] excel in tasks like __machine translation and text generation__. 
- [x] famous models such as GPT and BERT employ transformer architectures.

## Shallow neuristics

- [x] __simple rules or low-complexity methods__ used to solve problems or make decisions. 
- [x] can be quick to apply but may also yield __approximate results__.

# Points of interest

## Steps of LMs' training

1. __Data collection__.
2. __Preprocessing:__ cleaning and formatting the text data, including __tokenization__ (breaking text into individual words or subwords), handling special characters, and potentially applying techniques like stemming or lemmatization.
    a. __stemming:__ reducing words to their root by removing suffixes. e.g. "eating", "eats", "eaten" become "eat". Fast but may produce imperfect results as it does not consider context.
    b. __lemmatization:__ reducing words to their canonical form. e.g. "better" become "good", "running" become "run".
3. __Token embedding:__ converting tokens into __vectors__ that can be understood by the model. Involves techniques like __word embeddings__ (e.g., Word2Vec, GloVe) or __subword embeddings__ (e.g., Byte Pair Encoding, SentencePiece).
4. __Model training:__ involves feeding the tokenized and embedded text into the model and adjusting its parameters (e.g., weights in neural networks) iteratively to minimize prediction errors.
    a. __optimization algorithms__: improve the model's __ability to generate coherent and contextually relevant text__, while minimizing errors and maximizing language understanding. 
5. __Evaluation:__ assessing the performance of the trained model using various metrics such as __perplexity, accuracy, or BLEU score__ (Bilingual Evaluation Understudy).
6. __Fine-tuning__ (optional): fine-tuning the pre-trained model on a __specific task or domain__ to improve its performance for a particular application.
7. __Deployment__.

## Transfer learning

- [x] transferring a pre-trained model's learned knowledge to a new related task or domain.
- [x] pre-trained model is used as a starting point.
- [x] allows saving time + resources.

# Tasks

## Task 1: Create a synthetic dataset

Generate a train (size: 70) & a test dataset (size: 30):
```
    Dtrain = { (xi, yi): 1 <= i <= 100 ^ (i % 10) \notin { 1, 3, 7 }}
    Dtest = { (xi, yi): 1 <= i <= 100 ^ (i % 10) \in { 1, 3, 7 }}
```

<!-- ##### Train set
- [ ] various instruction / sentence
- [ ] various instruction's length
- [ ] add noise + mistakes (e.g. spelling mistakes)
- [ ] reverse order

##### Test set -->

ðŸ“š Doc:
- [Create dataset](https://huggingface.co/docs/datasets/create_dataset)
- stackoverflow
- GPT-3.5

### Task 2: Evaluate models

ðŸ“š Doc:
- [Evaluate predictions](https://huggingface.co/docs/datasets/metrics)

<!-- # Improvements -->

